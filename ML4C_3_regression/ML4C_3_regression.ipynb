{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Supervised Learning - Regression\n",
        "In the first tutorial you worked on binary classification to predict whether a point should be one class or another. In the second, you extended to multiple classes to predict a digit that the hand-drawn image corresponded to. But what if you wanted to predict a number? Let's say you wanted to predict a housing price. Knowing what you currently know, you might think about bucketizing prices. Maybe you end up with 100 buckets between \\$400,000 - \\$1,000,000. This would result in 100 classes where class 0 represents prices `[400,000, 406,000)`, class 1 represents `[406,000, 412,000)`, etc. But what do you do once you're in a bucket? You still need an actual value. You could randomly pick some value, but you might end up leaving money on the table. Also think about what this means. This would mean you've set a minimum and maximum range for a housing prediction. Your classifier would have to select something. What if a house is valued at \\$12m and your model says it's worth \\$1m? This is where regression comes in. Regression works with continuous values, while classification works with discrete labels. The goal of regression is to predict a continuous value rather than a label. Because of this, the goal is to minimize the error - the difference between the predicted value and the real value.\n",
        "\n",
        "### Ames Housing Data\n",
        "In this tutorial you will be working with the Ames housing market data. This is a very common dataset to work with when getting started with machine learning. Just like once upon a time you did a `hello world`  or a `Fizz Buzz` or a `fibonacci` program, you will get familiar with these \"starter\" datasets. Half moon and MNIST are also common (your first two tutorials).\n",
        "\n",
        "In this tutorial you will learn:<br/>\n",
        "- How to load data from openml\n",
        "- Data cleaning and preprocessing\n",
        "- How to handle categorical variables\n",
        "- Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)\n",
        "- Ensemble methods\n",
        "- How to visualize decision trees\n",
        "- One way to determine feature importance\n",
        "\n",
        "Let's get started!"
      ],
      "metadata": {
        "id": "FexoJWwk6dAD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PUUjb3IvPTY"
      },
      "outputs": [],
      "source": [
        "# This cell is meant to install dependencies within google colab\n",
        "def _install_deps():\n",
        "    try:\n",
        "        if 'google.colab' in str(get_ipython()):\n",
        "            print('Installing dependencies within Google Colab...')\n",
        "            !wget https://github.com/Chrispresso/ML-for-coders/blob/main/requirements.txt?raw=True -O requirements.txt\n",
        "            !python -m pip install -r requirements.txt\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "def _install_extra_deps():\n",
        "    try:\n",
        "        if 'google.colab' in str(get_ipython()):\n",
        "            import requests\n",
        "            # Grab the name of the notebook and download https://github.com/Chrispresso/ML-for-coders/tree/main/<filename>/requirements.txt\n",
        "            filename = requests.get('http://172.28.0.2:9000/api/sessions').json()[0]['name']\n",
        "            filename = filename[:filename.rindex('.')]\n",
        "            notebook_specific_reqs = f\"https://github.com/Chrispresso/ML-for-coders/tree/main/{filename}/requirements.txt?raw=True\"\n",
        "            !wget {notebook_specific_reqs} -O requirements_extra.txt\n",
        "            !python -m pip install -r requirements_extra.txt\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "try:\n",
        "    if __cell_install_requirements:\n",
        "        _install_deps()\n",
        "        _install_extra_deps()\n",
        "    else:\n",
        "        pass\n",
        "except:\n",
        "    _install_deps()\n",
        "\n",
        "__cell_install_requirements = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Data\n",
        "There are many publicly available datasets and servers where they reside. In this tutorial you will use `openml`. Specifically you will be using [this](https://www.openml.org/search?type=data&sort=runs&id=42165) dataset. Below you will load the dataset into a pandas dataframe."
      ],
      "metadata": {
        "id": "UoGfpkcBlSJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "sklearn.__version__"
      ],
      "metadata": {
        "id": "H6J1Kh4pYtem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "import pandas as pd\n",
        "\n",
        "data = fetch_openml(data_id=42165, as_frame=True)\n",
        "column_names = data.feature_names\n",
        "df = pd.DataFrame(data.data, columns=column_names)\n",
        "df['Price'] = data.target"
      ],
      "metadata": {
        "id": "JKYggi0xvrno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And that's all that's needed to load the data. Luckily Python has many packages to help make loading the data fairly easy. I  wanted to also introduce `openml` since it can be a good place to find datasets. Again, a lot of the problems you will work on can first be tested on publicly available datasets."
      ],
      "metadata": {
        "id": "opRC_gkOqnEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Cleaning and Preprocessing\n",
        "Data cleaning and data preprocessing are some of the most important things you'll do when creating a machine learning solution. Machine learning methods consume tons of data. If the data is bad, the model will be too. I can't possibly cover all the things you'll want to look at in terms of data cleaning and preprocessing in a single tutorial, so I'll be revisiting these throughout the tutorials.\n",
        "\n",
        "Let's start by getting some information on our newly created [dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)."
      ],
      "metadata": {
        "id": "2VaVRCE8rEzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "CXn16FFNvspU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see you have 80 features and the target price in the dataframe. Some of the features are floats and others are objects. You can also see that a number of features have missing values. Notice that there are a total of 1460 entries, so any feature that states a different value of 1460 in the non-null count column will contain some missing data.\n",
        "\n",
        "The features that are `float64` are basically what you've seen in previous tutorials - some real number. Let's take a look at something like `GarageType` instead and get the different values."
      ],
      "metadata": {
        "id": "mLvfQg9Tz1o_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['GarageType'].unique()"
      ],
      "metadata": {
        "id": "_FtelmMPwlc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are seven categorical values. These need to get converted to a numerical value in order to be interpreted by a model. There's a number of ways to do this. The first is to use one-hot encoding. For `GarageType`, this would create a vector of length 6 and place a `1` at the location representing the `GarageType` for that entry, and a `0` elsewhere. For instance, if a certain house had an `Attchd` garage, this would be represented by `[1, 0, 0, 0, 0, 0, 0]`.\n",
        "\n",
        "Another option would be an ordinal encoder. This would replace the categoroical value with an integer value representing the location in the original array. For instance, if a house had `Basment` garage, this would be represented by `6`.\n",
        "\n",
        "Another option would be to slap on some weights to an embedding layer and allow a model to learn the embeddings of the categorical values. This is something we'll look at in the future, but is a bit complex for now. One of the things we're after is ensuring that all features get treated fairly. If we were to use the ordinal encoder, then we run into a problem where `CarPort` will have a higher value than `Attchd` simply because of the location in the array. So instead we'll be using a one-hot encoder to essentially say whether a certain feature value is on or off.\n",
        "\n",
        "Cool, so now you have an idea of what you can do with categorical features. But before we handle categorical features we need to talk about the elephant in the room - missing values. What do we do with missing values? Let's discuss it by first looking at `FireplaceQu`"
      ],
      "metadata": {
        "id": "Z4P4Bx-53ecZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_copy = df.copy()\n",
        "df_copy['FireplaceQu'].value_counts(dropna=False).plot(kind='bar')"
      ],
      "metadata": {
        "id": "_-Vhzh5c2wu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, one option we have is to remove entries with missing values. This isn't great in this case because most of the entries would be gone. We can also try to be intelligent and use `Fireplaces` to help fill in the missing values. We can assume that if they have no fireplaces, there won't be a fireplace quality. Let's see what happens."
      ],
      "metadata": {
        "id": "LzjA4LfMnChm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy['FireplaceQu'] = df_copy['FireplaceQu'].fillna('None')\n",
        "df_copy['FireplaceQu'].value_counts(dropna=False).plot(kind='bar')"
      ],
      "metadata": {
        "id": "v_QbDDxupMsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, making this change would keep the same distribution as before. That's good. You wouldn't want to make a change that resulted in `Fa` suddenly having a higher frequency that `Gd`.  But now let's look at `Fence`."
      ],
      "metadata": {
        "id": "75t1O4aFsk8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy['Fence'].value_counts(dropna=True).plot(kind='bar')"
      ],
      "metadata": {
        "id": "iQcAY8ubtUj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's hard to know if any of the other features would give us insight into whether or not a house has a fence. We could also train a model to take other features and predict whether or not the house has a fence. This might work but there are a lot more missing values than filled in values, so we probably wouldn't gain much insight.\n",
        "\n",
        "There's a couple more options. One option would be to sample from the distribution and randomly assign the missing values. This option can work well, but again, there's a lot of missing values here. A second option would be to just remove this feature entirely. This is the route I'm choosing to take. Let's start by removing the features that have too many missing values."
      ],
      "metadata": {
        "id": "HtQM8DvPtqs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# $MODIFY 1\n",
        "threshold = 0.85  # Percentage of values that must be filled in for a feature in order to include the feature\n",
        "cols_to_remove = list(df.columns[df.isna().sum() > (1 - threshold) * len(df) ])\n",
        "cols_to_remove"
      ],
      "metadata": {
        "id": "4ZhtReG0viGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So even though we could replace the missing values in `FireplaceQu`, there's enough missing values that it gets flagged for removal. Honestly that's fine since there are so many missing values. \n",
        "\n",
        "Next we want to look for features that have primarily zero value. Before this we looked at missing values - values that just weren't entered. But in this case we're looking for values that have been entered and are zero. If there's enough values for a feature that are zero, again, we might not gain much insight into the meaning of that feature. For instance, you and I know that everyone loves a nice kitchen. If there were a `KitchenSqft` feature and 90% of data were 0, how would we expect a model to be able to know what to do when it finds non-zero values?\n",
        "\n",
        "Sometimes in machine learning literature you'll find terms related to \"signal to noise ratio\". We want a good signal to noise ratio (SNR). This essentially means we expect to extract a good amount of signal compared to the background noise. If 90% of the data is 0 and 10% is non-zero, then the SNR is most likely going to be quite poor, and could hurt model performance. Let's make these modifications now."
      ],
      "metadata": {
        "id": "6wBTJSO7MZJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# $MODIFY 2\n",
        "threshold_zero_vals = 0.75  # Percentage of values that must be non-zero\n",
        "\n",
        "additional_cols_to_reomve = []\n",
        "\n",
        "for col in df.select_dtypes(include= 'float').columns:\n",
        "    count = df[col].count()\n",
        "    try:\n",
        "        count_zeros = df[col].value_counts()[0.0]\n",
        "    except:\n",
        "        count_zeros = 0.0\n",
        "    ratio_zeros = count_zeros / float(count)\n",
        "    \n",
        "    if ratio_zeros > threshold_zero_vals:\n",
        "        additional_cols_to_reomve.append(col)\n",
        "\n",
        "print(additional_cols_to_reomve)"
      ],
      "metadata": {
        "id": "5v5hP7J8POmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now remove these columns from the dataframe."
      ],
      "metadata": {
        "id": "n_DJ6gjRZHpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drop_cols = list(set(cols_to_remove) | set(additional_cols_to_reomve))\n",
        "df_new = df.drop(drop_cols, axis=1).copy()\n",
        "df_new.info()"
      ],
      "metadata": {
        "id": "HcHls0OyZUPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is looking a lot better. But we're not done yet! Yes, we removed features tha thave a large number of missing and zero values, but we also need to look at numeric features that still contain missing values."
      ],
      "metadata": {
        "id": "ZgvL6xIlklr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_new_numeric_cols = df_new.select_dtypes(include= 'float').columns\n",
        "missing_num_cols = list(df_new_numeric_cols[df_new.select_dtypes(include= 'float').isna().sum() > 0])\n",
        "missing_num_cols"
      ],
      "metadata": {
        "id": "hDCyRjgh8ulA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with `GarageYrBlt`. You might be tempted to just fill the missing values with 0, but that has a high chance to shift the distribution. Let's take a look to see what I mean."
      ],
      "metadata": {
        "id": "MTIKMO0SuK0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "if 'GarageYrBlt' in missing_num_cols:\n",
        "    og_garage = df_new['GarageYrBlt'].rename('OG')#.value_counts().rename('OG')\n",
        "    mod_garage = df_new['GarageYrBlt'].fillna(0.0).rename('MOD') #.value_counts().rename('MOD')\n",
        "    garage = pd.concat([og_garage, mod_garage], axis=1)\n",
        "\n",
        "    sns.displot(garage, kind=\"kde\")\n",
        "\n",
        "    print('og mean', og_garage.mean())\n",
        "    print('og std', og_garage.std())\n",
        "    print('mod mean', mod_garage.mean())\n",
        "    print('mod std', mod_garage.std())\n"
      ],
      "metadata": {
        "id": "4gw33GRhuix0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the above change would drastically change the original distribution. Instead, let's see what happens if we just assume that if there isn't a year built for the garage, then it must have been built \"when the house was built."
      ],
      "metadata": {
        "id": "I00tElY8jaNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if 'GarageYrBlt' in missing_num_cols:\n",
        "    og_garage = df_new['GarageYrBlt'].rename('OG')\n",
        "    garage_missing_idxs = df_new['GarageYrBlt'].isna()\n",
        "    mod_garage = df_new['GarageYrBlt'].rename('MOD')\n",
        "    mod_garage[garage_missing_idxs] = df_new['YearBuilt'][garage_missing_idxs]\n",
        "    # mod_garage = df_new['GarageYrBlt'].fillna(0.0).value_counts().rename('MOD')\n",
        "    garage = pd.concat([og_garage, mod_garage], axis=1)\n",
        "\n",
        "    sns.displot(garage, kind=\"kde\")\n",
        "\n",
        "    print('og mean', og_garage.mean())\n",
        "    print('og std', og_garage.std())\n",
        "    print('mod mean', mod_garage.mean())\n",
        "    print('mod std', mod_garage.std())"
      ],
      "metadata": {
        "id": "PNr3d7nRe2--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's much better. The distribution changes slightly, but still maintains a lot of the original information. Now let's take a look at `MasVnrArea` and see what happens if we replace the missing values with 0."
      ],
      "metadata": {
        "id": "y8ykHuXbchvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'MasVnrArea' in missing_num_cols:\n",
        "    og_mas_vnr_area = df_new['MasVnrArea'].rename('OG')\n",
        "    mod_mas_vnr_area = df_new['MasVnrArea'].fillna(0.0).rename('MOD')\n",
        "    mas_vnr_area = pd.concat([og_mas_vnr_area, mod_mas_vnr_area], axis=1)\n",
        "\n",
        "    sns.displot(mas_vnr_area, kind=\"kde\")\n",
        "\n",
        "    print('og mean', og_mas_vnr_area.mean())\n",
        "    print('og std', og_mas_vnr_area.std())\n",
        "    print('mod mean', mod_mas_vnr_area.mean())\n",
        "    print('mod std', mod_mas_vnr_area.std())\n"
      ],
      "metadata": {
        "id": "3OskCBNTS2Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That doesn't seem to have much effect on it, so that modification will be fine.\n",
        "\n",
        "Now let's commit to these modification and change them within our dataframe."
      ],
      "metadata": {
        "id": "Nz1Urr8sj7-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'GarageYrBlt' in missing_num_cols:\n",
        "    garage_missing_idxs = df_new['GarageYrBlt'].isna()\n",
        "    df_new.loc[garage_missing_idxs, 'GarageYrBlt'] = df_new['YearBuilt'][garage_missing_idxs]\n",
        "\n",
        "if 'MasVnrArea' in missing_num_cols:\n",
        "    df_new['MasVnrArea'] = df_new['MasVnrArea'].fillna(0.0)\n"
      ],
      "metadata": {
        "id": "nzmkisrKbg-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for the data we need to create a one-hot encoding for the categorical values. There are a couple of ways to do this. One would be to use `pd.get_dummies` which creates dummy values (one-hot). Another way is to use `sklearn` and to use their `OneHotEncoder`. I'm choosing to use that latter because it follows more of a supervised learning approach that you'll often see with these models. First we need to fill the missing values of categories. We don't need to do anything fancy here, but the one-hot encoder will have issues with a value that's missing. A simple way to correct this is to replace all missing categorical values with \"missing\"."
      ],
      "metadata": {
        "id": "0COb4RzLlfuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = list(df_new.select_dtypes(include='object'))\n",
        "df_new[cat_cols] = df_new[cat_cols].fillna('missing')"
      ],
      "metadata": {
        "id": "bTR51rBHlesy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create a one-hot encoding."
      ],
      "metadata": {
        "id": "bS_TxIW0ojdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "one_hot_encoder = OneHotEncoder().fit(df_new[cat_cols])"
      ],
      "metadata": {
        "id": "XFrhp3dZmDPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This can be a common approach in supervised learning. Less so these days, but most of the sklearn package follows a similar format, which is why I chose to use it. Sklearn is still a wildly popular package and for good reason. For these approaches you will generally see `.fit()`, `.fit_transform()` and `.transform()`.\n",
        "\n",
        "`.fit()` simply fits a model to the data provided.<br/>\n",
        "`.fit_transform()` will fit and then modify the data. For instance in this case `.fit()` is really only concerned with figuring out what the values of the categories are, but the transformation is what actually modifies the values.<br/>\n",
        "`.transform()` takes the underlying model that has already been learned and applies the transformation to the data.\n",
        "\n",
        "These probably seem like the same thing right now but there are subtle differences. If we were splitting this data ahead of time and running `.fit()` on the training data, we can also run `.transform()` (or simply combine them into `.fit_transform()`). However, with the test set, you would use just `.transform()`. Why? Because we don't know the distribution of data for the test set. We can assume that it's the same as the training data, but we can't know for sure. \n",
        "\n",
        "Next let's create standardize the numerical features. We want each feature/column to have similar importance when given to the model. If we left everything as-is, then features that naturally have higher values would be more effected by small weight changes. For instance, the model might bias towards square feet instead of number of bedrooms, since their range of values is so different."
      ],
      "metadata": {
        "id": "optVjfa_o53p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# First remove Price.\n",
        "target = df_new['Price'].copy()\n",
        "df_new = df_new.drop(['Price', 'Id'], axis=1)\n",
        "\n",
        "# Grab the numerical columns and fit a standard scaler\n",
        "num_cols = list(df_new.select_dtypes(include='float'))\n",
        "standard_scaler = StandardScaler().fit(df_new[num_cols])"
      ],
      "metadata": {
        "id": "sL8gspGUo4lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One final look at the dataframe before we transform it:"
      ],
      "metadata": {
        "id": "KP5WXZXmtgHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.head()"
      ],
      "metadata": {
        "id": "Wj4l5Hakrzym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can transform the data and create a new dataframe with that transformed data."
      ],
      "metadata": {
        "id": "j2CKO7nAuP4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Combine the data by stacking along the columns\n",
        "combined_data = np.column_stack((\n",
        "    one_hot_encoder.transform(df_new[cat_cols]).todense(), \n",
        "    standard_scaler.transform(df_new[num_cols])\n",
        "))\n",
        "\n",
        "# Create the new column names\n",
        "new_cols = list(one_hot_encoder.get_feature_names_out()) + num_cols\n",
        "\n",
        "X = pd.DataFrame(combined_data, columns=new_cols)\n",
        "X.head()"
      ],
      "metadata": {
        "id": "kmnyjjt1tj6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Awesome! You're done with data preprocessing now. You just created one-hot encoded variables for your categorical data and used a standard scaler across numerical features. You're now left with more columns than you started with, but your data is now in a good state for machine learning models!"
      ],
      "metadata": {
        "id": "H6hjXPzV3uyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MSE\n",
        "\n",
        "In the past you were doing classification. The loss functions that you dealt with previously are concerned with whether or not a predicted class is the correct one. This changes a bit for predicting a real value. You now want to minimize the error of the model. In a sense you want to minimize the difference between the predicted value and the actual - this is where MSE comes in. MSE forms a loss that is equal to the sum over all training examples of the squared difference.\n",
        "\n",
        "Technically it looks like this:\n",
        "\n",
        "$$\n",
        "MSE = \\frac{1}{n} ∑_{i=1}^{n}{(Y_i - \\hat{Y}_i)}^2\n",
        "$$\n",
        "\n",
        "where $Y_i$ is the actual value and $\\hat{Y}_i$ is the predicted.\n",
        "Now get ready cause this will blow your mind. RMSE is the same thing.... but you take a square root after.\n",
        "\n",
        "i.e. $$\n",
        "RMSE = \\sqrt{MSE}\n",
        "$$\n",
        "\n",
        "It's good to use RMSE when you want to know how well your model is fitting a dataset since it will be in the same units.\n",
        "\n",
        "For instance, say I wanted to predict the number of cookies I can eat. If the actual is 12 but I predicted 14 then MSE would be $(12 - 14)^2 = 4$. Using RMSE it would be $\\sqrt{4} = 2$. This is helpful because if I did this a number of times I would end up with the average deviation between predicted and actual."
      ],
      "metadata": {
        "id": "ygUYVL-h4hac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Feature Importance"
      ],
      "metadata": {
        "id": "YdWn8QYX8FRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by splitting the training set into a train and test set."
      ],
      "metadata": {
        "id": "ssFbliAr8HT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, target, random_state=0xC0FFEE)"
      ],
      "metadata": {
        "id": "rKH3f9QLwiD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous two tutorials you used PyTorch to solve your problems. Technically you could use PyTorch here as well, but it's not needed. Sometimes it's faster or even better to apply simple solutions. Sklearn offers quite a few traditional and advanced machine learning techniques and we're going to explore a few. I'll introduce one and you can explore some more in the tasks section. The one I'll introduce is `RandomForestRegressor` which is an `ensemble` method. Ensemble methods work by having a number of learners - often trained with different parameters or data. These learners each have their own prediction, or model output. The goal of ensemble methods is to combine the predictions of these learners into a final prediction. Generally these are done either with `averaging` or with `boosting`.\n",
        "\n",
        "With averaging methods, each learner is trained independently and then you average their prediction at the end. On the other hand, boosting methods are built sequentially, where learner 2 would try to reduce the bias of learner 1. This effectively makes learner 2 try to focus on harder examples. At the end a prediction is made via a weighted majority vote.\n",
        "\n",
        "We'll be looking at an averaging method based on randomized decision trees called `RandomForest`."
      ],
      "metadata": {
        "id": "P3SDecnj6H4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "regressor = RandomForestRegressor()\n",
        "regressor.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "LMhg7VTj8amp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the score that the model gets."
      ],
      "metadata": {
        "id": "ms4vkXGCEUuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(regressor.score(X_train, y_train))\n",
        "print(regressor.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "BEv-fXFA9Ow2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way to get around this is to set a minimum number of samples per leaf node. A great thing about decision trees is that you can use them to know **why** your model is predicting something. At some point you might want to be able to explain why your model is choosing what it's choosing. Imagine you made a prediction if someone gets a car loan. If they don't, you don't really want to say \"well my black-box model said no\". It would be better to trace it through and find a reason to give the customer."
      ],
      "metadata": {
        "id": "L8dfsFmlEozO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "\n",
        "regressor = RandomForestRegressor(min_samples_leaf=20)\n",
        "regressor.fit(X_train, y_train)\n",
        "print(regressor.score(X_train, y_train))\n",
        "print(regressor.score(X_test, y_test))\n",
        "\n",
        "estimator = regressor.estimators_[0]\n",
        "plt.figure(figsize=(80,30))\n",
        "tree.plot_tree(estimator, feature_names = X.columns, filled=True, rounded=True, fontsize=12)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "wumKYKCOEtwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see a higher score for the train set than the test set. This is a sign that there might be some overfitting. Let's look at feature importance. Luckily our `RandomForestRegressor` has a built in `feature_importances_` attribute that will tell us the score. Let's see the **least** important features first."
      ],
      "metadata": {
        "id": "w0UzASGlEXZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bottom_10 = list(regressor.feature_importances_.argsort())[:10]\n",
        "print(np.array(new_cols)[bottom_10])"
      ],
      "metadata": {
        "id": "hC7Gfvju9SAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well that's.... not helpful. We have our one-hot encoded variables in there but not being treated as a whole. What we want is a way to look at feature importance without the dummy values, i.e. ` 'Exterior2nd_CBlock' 'RoofMatl_ClyTile' 'RoofMatl_Roll', 'Exterior2nd_Other'` would instead be `'Exterior2nd', 'RoofMatl'`. Sklearn offers a utility here! It's called `permutation_importance`. Basically it takes a model and predicts a score for it. Then it permutes a feature column and calculates the score again. The difference between the two is the permutation importance. Now, this is a bit tricky because if we permute the feature as they are, we're going to end up in the same boat. We want to essentially permute the features before the categorical features become one-hot encoded. To do this we can use a `Pipeline`. Machine learning pipelines are generally used in production to help make things uniform and to simplify the process. We're going to use a `Pipeline` here to tell sklearn that we want it to perform the encoding and standardization. This way it can permute the features **before** running the pipeline."
      ],
      "metadata": {
        "id": "VPdrz6xOFIfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import *\n",
        "from sklearn.linear_model import *\n",
        "\n",
        "# Train/test split on the raw dataframe\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_new, target, random_state=0xC0FFEE)"
      ],
      "metadata": {
        "id": "VdKo_ZlGB3-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical pipeline\n",
        "cat_pipe = Pipeline(\n",
        "    [\n",
        "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Numerical pipeline\n",
        "num_pipe = StandardScaler()\n",
        "\n",
        "# Define the column transformer which will run the numerical and categorical\n",
        "# pipelines that we defined above\n",
        "preprocessing = ColumnTransformer(\n",
        "    [\n",
        "        ('cat', cat_pipe, cat_cols),\n",
        "        ('num', num_pipe, num_cols)\n",
        "    ],\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "\n",
        "# The full pipeline - just the preprocessing and then the regressor\n",
        "pipeline = Pipeline(\n",
        "    [\n",
        "        ('preprocess', preprocessing),\n",
        "        # $MODIFY 3\n",
        "        ('reg', RandomForestRegressor(random_state=0xC0FFEE))\n",
        "    ]\n",
        ")\n",
        "\n",
        "# $MODIFY 4 - have some fun dawg\n",
        "# Remember, the  set_params() takes in a name followed by double underscore\n",
        "# and then the attribute you want to change. In this case I'm using 'reg'\n",
        "# since I named the regressor 'reg'.\n",
        "pipeline.set_params(reg__min_samples_leaf=20)\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "68Edtroj9faj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that in the pipeline we're not specifying a loss function to minimize like we did with PyTorch. Instead, this is usually passed as a `criterion` parameter (depending on the model you select) and most of them use MSE by default.\n",
        "\n",
        " Now we can check how the model performs on the training and test data."
      ],
      "metadata": {
        "id": "u1ApSnDBxKIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('train score', pipeline.score(X_train, y_train))\n",
        "print('test score', pipeline.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "UMAoN-emxTTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal is to get the test score somewhere around the train score. We don't want to be overfitting the train set. Let's now take a look at the feature importance calculated over the test set."
      ],
      "metadata": {
        "id": "XM-fJxtjMBQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "result = permutation_importance(\n",
        "    pipeline, X_test, y_test, n_repeats=5, random_state=0xC0FFEE, n_jobs=10\n",
        ")\n",
        "\n",
        "sorted_importances_idx = result.importances_mean.argsort()\n",
        "most_important = list(sorted_importances_idx[-10:])[::-1]\n",
        "least_important = list(sorted_importances_idx[:5])[::-1]\n",
        "idxs = np.array(most_important + least_important)\n",
        "importances = pd.DataFrame(\n",
        "    result.importances[idxs].T,\n",
        "    columns=df_new.columns[idxs],\n",
        ")\n",
        "\n",
        "ax = importances.plot.box(vert=False, whis=10, figsize=(12, 8))\n",
        "ax.set_title(\"Permutation Importances (test set)\")\n",
        "ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
        "ax.set_xlabel(\"Decrease in accuracy score\")\n",
        "ax.figure.tight_layout()\n"
      ],
      "metadata": {
        "id": "Vi24hz_ZK-ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's take a look at the feature importance of the training set. These should be similar. If the training set importance is drastically different, there's a good chance you overfit to the training set and the model didn't generalize well."
      ],
      "metadata": {
        "id": "gH8VGkELGVuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = permutation_importance(\n",
        "    pipeline, X_train, y_train, n_repeats=5, random_state=0xC0FFEE, n_jobs=10\n",
        ")\n",
        "\n",
        "sorted_importances_idx = result.importances_mean.argsort()\n",
        "most_important = list(sorted_importances_idx[-10:])[::-1]\n",
        "least_important = list(sorted_importances_idx[:5])[::-1]\n",
        "idxs = np.array(most_important + least_important)\n",
        "importances = pd.DataFrame(\n",
        "    result.importances[idxs].T,\n",
        "    columns=df_new.columns[idxs],\n",
        ")\n",
        "ax = importances.plot.box(vert=False, whis=10, figsize=(12, 8))\n",
        "ax.set_title(\"Permutation Importances (train set)\")\n",
        "ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
        "ax.set_xlabel(\"Decrease in accuracy score\")\n",
        "ax.figure.tight_layout()"
      ],
      "metadata": {
        "id": "uhJRJHxSM0G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's take a look at some of the predicted values vs the actual ones."
      ],
      "metadata": {
        "id": "cmUMdIUDHhU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "y_pred = pipeline.predict(X_test)\n",
        "y_actual = y_test.to_numpy()\n",
        "\n",
        "nshow = len(y_actual)\n",
        "\n",
        "\n",
        "df_outcome = pd.DataFrame(\n",
        "    np.column_stack((\n",
        "        y_pred[:nshow],\n",
        "        y_actual[:nshow],\n",
        "        y_pred[:nshow] - y_actual[:nshow]\n",
        "    )),\n",
        "    columns=['pred', 'actual', 'delta']\n",
        ").round()\n",
        "\n",
        "\n",
        "print(f'MSE: {mean_squared_error(y_actual, y_pred, squared=True)}')\n",
        "print(f'RMSE: {mean_squared_error(y_actual, y_pred, squared=False)}')\n",
        "\n",
        "print('\\n=== Predictions ===')\n",
        "print(df_outcome)\n",
        "print()\n",
        "\n",
        "print('Average absolute difference', df_outcome['delta'].abs().mean())\n",
        "print('Average difference', df_outcome['delta'].mean())"
      ],
      "metadata": {
        "id": "J5HTilC2cNrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tasks\n",
        "\n",
        "1. If you change `threshold` and re-run the code, how does it effect the model? Is there a threshold that seems better?\n",
        "\n",
        "2. If you change `threshold_zero_vals` and re-run the code, how does it effect the model? Also, think about this. We are thresholding on values that are mostly zero. Should we also threshold on values that are mostly the same? For instance, if a feature has 90% of its values as `2`, does it really matter if we use it?\n",
        "\n",
        "3. Try changing `RandomForestRegressor` to a different kind of regressor. A list of supported sklearn ensemble methods can be found [here](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble). Try to read through some of them and try out a few. How do they compare? A list of linear models and their regressors can be found [here](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model). Read through some of them and try out a few.\n",
        "\n",
        "4. This can be done in conjunction with task 3, but try to change some of the parameters for the regressor(s) of your choice. How do these parameters effect the performance?\n",
        "\n",
        "5. If you made it this far consider taking [the survey](https://www.surveymonkey.com/r/MBZCZMT). Results are anonymous and help me improve future tutorials."
      ],
      "metadata": {
        "id": "nbsIcn77xaMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "In this tutorial you saw how to load datasets using openml. You also saw how to go about data preprocessing and cleaning - a very important part of all machine learning applications. You learned several ways to handle categorical features and how to use one-hot encoding specifically to help. You learned about ensemble methods and how to use several types of regressors to fit your data and even saw how to plot the decision tree that your model chose! Finally you saw how to look at feature importance to measure how important a particular feature is for model accuracy.\n",
        "\n",
        "That's all for this one, see you next time!"
      ],
      "metadata": {
        "id": "6n3hNl-T5Jbv"
      }
    }
  ]
}