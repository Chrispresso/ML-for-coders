{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmUhaxngdgXZ"
   },
   "source": [
    "## Supervised Learning - Multiclass Classification\n",
    "In the last tutorial you learned how to use the [binary cross entropy](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) loss function to separate two classes. In this tutorial you'll be using [cross entropy](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) loss to learn to predict multiple classes.\n",
    "\n",
    "Specifically, you will be working with the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset which contains 70k hand written digits from 0-9. In the last tutorial I had you overfit the training data to help illustrate how a neural network can learn to fully approximate some unknown function. Although that's cool, it's also error prone. The problem comes when the neural network has a new data point that it hasn't seen before. If the neural network is overfit to the training data, it's possible that the new data point will be misclassified since the decision boundary will be so tightly fit to the training data. Because of this, you will use 60k images to train on and 10k images to test on.\n",
    "\n",
    "IMPORTANT: this tutorial was designed to be run on Google Colab. It doesn't need to be, but if you're behind a firewall you'll most likely face issues when downloading the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tYnIrVt47p8"
   },
   "source": [
    "## MNIST\n",
    "In this tutorial you will learn:<br/>\n",
    "- How to load the training and test data\n",
    "- How to standardize your data\n",
    "- How to create a deep neural network of arbitrary depth\n",
    "- How to use other activation functions such as ReLU\n",
    "- How to determine if you're overfitting the training data and what you can do to prevent that\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1fa3MJn51RO"
   },
   "source": [
    "### Load the data\n",
    "\n",
    "First you'll load the training and test dataset via PyTorch. PyTorch has [quite a few](https://pytorch.org/vision/stable/datasets.html) datasets to choose from which makes it easy to test models. At some point you'll want to load data that isn't publicly available, but I often find that there's a similar open-sourced problem which makes it easy to test ideas before commiting to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0N4UdCQdJ_y"
   },
   "outputs": [],
   "source": [
    "# This cell is meant to install dependencies within google colab\n",
    "def _install_deps():\n",
    "    try:\n",
    "        if 'google.colab' in str(get_ipython()):\n",
    "            print('Installing dependencies within Google Colab...')\n",
    "            !wget https://github.com/Chrispresso/ML-for-coders/blob/main/requirements.txt?raw=True -O requirements.txt\n",
    "            !python -m pip install -r requirements.txt\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    if __cell_install_requirements:\n",
    "        _install_deps()\n",
    "    else:\n",
    "        pass\n",
    "except:\n",
    "    _install_deps()\n",
    "\n",
    "__cell_install_requirements = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7cUsKaonEZ2x"
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from IPython import display \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "train_set = datasets.MNIST(root='data', train=True, download=True)\n",
    "test_set = datasets.MNIST(root='data', train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qL-QaAvNceIp"
   },
   "source": [
    "Let's look at a sample of the training set so we know what we'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzRuSTJBEnJI"
   },
   "outputs": [],
   "source": [
    "pair = train_set[0]\n",
    "pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lit-Fksncy3-"
   },
   "source": [
    "So each entry in the dataset is a tuple containing the image and the label (0-9). Take a look at the size of the image. It's 28x28. In this case all images here are 28x28.\n",
    "\n",
    "In the previous tutorial you worked with an x-position and a y-position and predicted a class. In that case you dealt with 2 features: the (x,y) pair. In this case you have 28x28 pixels and will use each pixel as it's own feature. That's 784 features! Let's now take a look at one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSPonezi9Od9"
   },
   "outputs": [],
   "source": [
    "img = pair[0]  # Extract the image\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKLwHNkv995c"
   },
   "source": [
    "Okay! So you can see a \"5\" written above. Computers don't understand images very well but they do a good job with numbers. Because of this you will be using the raw pixel values. Let's take a look what this \"5\" is to a computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k38ONs7H-RzR"
   },
   "outputs": [],
   "source": [
    "print('shape is:', np.array(img).shape)\n",
    "print('min value:', np.min(np.array(img)))\n",
    "print('max value:', np.max(np.array(img)))\n",
    "print('10x10 section of the number \"5\" above:')\n",
    "print(np.array(img)[10:20, 10:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vR9QK6O-_sy"
   },
   "source": [
    "This might look overwhelming but just take it in one piece at a time. This means you'll be using a 28x28 matrix that has values 0-255. \n",
    "\n",
    "### Standardize the Data\n",
    "\n",
    "Neural networks use small weight values, so having large inputs such as 255 can cause issues during training. Because of this there are two common ways to modify features. The first is with normalization. Normalizing your data means to convert it to a given range. This range is generally `[-1, 1]` or `[0, 1]`. The smaller values tend to increase training speed and help reduce numerical instability. The second way is called standardization. Standardizing your data consists of shifting the data to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "Because our image is already bounded by `[0, 255]`, reducing it via normalization will keep the same spread. This would allow for faster and more stable training, but we're instead going to use standardization. I often find standardizing the data more reliable but you can try both methods and see which works better! (This is a common trend in machine learning)\n",
    "\n",
    "Now let's figuring out what the mean and standard deviation of our training data is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LnZQPEGXAAPN"
   },
   "outputs": [],
   "source": [
    "X = np.stack([np.array(pair[0]) for pair in train_set])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1u5x5cPHALy3"
   },
   "outputs": [],
   "source": [
    "np.mean(X / 255.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2-Q1WlnAM9W"
   },
   "outputs": [],
   "source": [
    "np.std(X / 255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLn9h7ekK7SN"
   },
   "source": [
    "Woah, woah, woah. All that talk about not normalizing the data and suddenly we're dividing by 255? Since normalizing the data keeps the same spread, it doesn't matter if we subtract the un-normalized or normalized mean as long as we're consistent. That means we'll also want to first transform the data to the range `[0, 1`] and **then** apply the standardization. In this case we'll want to subtract the mean and divide by the standard deviation we see above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJ40PDgaAeWF"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts to a Float Tensor and divides by 255\n",
    "    # $MODIFY 1\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Mean and std\n",
    "])\n",
    "train_set = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "test_set = datasets.MNIST(root='data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70ToKBtJMzvc"
   },
   "source": [
    "Notice that you're applying the same transformation to the training set as you are the test set. Your test set is a hidden set - you are never allowed to look at it. The transformations you apply to the training set should be the same as to the test set. You're under the assumption your training data is coming from a similar distribution that your unseen data is coming from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lbyulp3E_E5x"
   },
   "source": [
    "### Deep Neural Networks\n",
    "\n",
    "Next you're going allow the ability to create neural networks of arbitray depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEF7xeqoCEQa"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, hidden_layers, activation_func):\n",
    "        super().__init__()\n",
    "        self.activation_func = activation_func\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Add a linear layer from input to the first hidden layer\n",
    "        self.layers.append(nn.Linear(784, hidden_layers[0]))\n",
    "        # Add arbitrary number of hidden layers\n",
    "        for i in range(len(hidden_layers) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_layers[i], hidden_layers[i+1]))\n",
    "        # Add a final one from the last hidden layer to the number of classes (10)\n",
    "        self.layers.append(nn.Linear(hidden_layers[-1], 10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers[:-1]:\n",
    "            out = layer(out)\n",
    "            out = self.activation_func(out)\n",
    "        # The final hidden layer to the classes don't need to go through \n",
    "        # an activation function\n",
    "        out = self.layers[-1](out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsHseyvAI684"
   },
   "source": [
    "This is pretty similar to what you saw in the last tutorial. The only difference is now you're using a [module list](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html) to hold the layers. Take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7rCPIuaMr2p"
   },
   "outputs": [],
   "source": [
    "net1 = Network([128, 64, 32], nn.ReLU())\n",
    "print(net1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3ObJZ-GP1x1"
   },
   "source": [
    "Using a module list is an efficient way to dynamically add modules (layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJ5-SRwUQzwu"
   },
   "source": [
    "### Activation Functions\n",
    "\n",
    "PyTorch offers many [non-linear activation functions](https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions). In the last tutorial you used the `Sigmoid` activation function but most modern models use a rectified linear unit (ReLU). There are generally two main reasons to favor ReLU over Sigmoid. The first is that ReLU is faster to compute, along with the derivative. The second is that Sigmoid often suffers from a vanishing gradient. As neural networks get sufficiently deep, it can be easy to saturate the activation function. This happens when the activation function plateaus, causing the derivative to be 0. Once the derivative is 0, the updates that get passed back will also be 0 resulting in an extremely slow convergence, if there even is one. You can read more about ReLU vs. Sgimoid activations [here](https://wandb.ai/ayush-thakur/dl-question-bank/reports/ReLU-vs-Sigmoid-Function-in-Deep-Neural-Networks--VmlldzoyMDk0MzI#:~:text=In%20other%20words%2C%20once%20a,eliminated%20by%20using%20leaky%20ReLUs.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-B39EzKbztL"
   },
   "source": [
    "### Training\n",
    "\n",
    "How do you avoid overfitting the training data if you're not allowed to touch the test set? A common practive is to have a validation set - a subset of the training set which you are allowed to look at, but treat as though it were a test set. By using the validation set you can take a look at how your model **might** act when given the test set. <br/>\n",
    "\n",
    "In fact, an easy way to see if you're overfitting the training data is to watch the training loss decrease while the validation loss begins to increase. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7DRITQKM-x4"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from torchmetrics import Accuracy\n",
    "import torch\n",
    "pl.seed_everything(0xC0FFEE)\n",
    "\n",
    "# Split the training set from 60k training to 50k/10k training/val\n",
    "train, val = torch.utils.data.random_split(train_set, [50000, 10000])\n",
    "print(type(train))\n",
    "print(len(train))\n",
    "print(len(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVmN4mbQgejY"
   },
   "source": [
    "Now it's time to create the lightning module. Very similar to what you did last time but now the loss function is `F.cross_entropy` instead of `F.binary_cross_entropy`. You're also now providing a `validation_step()` and `test_step()` function. This is similar to the training step you've seen benfore, but gets run automatically by PyTorch Lightning when it's time to validate and test the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cTgO0Km7c3xt"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.net = model\n",
    "       \n",
    "        self.val_accuracy = Accuracy()\n",
    "        self.test_accuracy = Accuracy()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X = X.view(-1, 28*28)\n",
    "        y_pred = self.net(X)\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        \n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "      \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X = X.view(-1, 28*28)\n",
    "        y_pred = self.net(X)\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        self.val_accuracy.update(y_pred, y)\n",
    "\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', self.val_accuracy, prog_bar=True)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X = X.view(-1, 28*28)\n",
    "        y_pred = self.net(X)\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        self.test_accuracy.update(y_pred, y)\n",
    "\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        self.log('test_acc', self.test_accuracy, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=2e-3)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0F0LlPxiuWW"
   },
   "source": [
    "Next, instantiate the model and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5drRvP4sk4b1"
   },
   "outputs": [],
   "source": [
    "lit_model = LitModel(\n",
    "    Network([32], nn.ReLU()) \n",
    ")\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader   = DataLoader(val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    enable_checkpointing=False,\n",
    "    callbacks=[TQDMProgressBar(refresh_rate=20)]\n",
    ")\n",
    "\n",
    "trainer.fit(lit_model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hkL9exUi4XJ"
   },
   "source": [
    "Now you can load `tensorboard` to take a look at the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_kuUqzVmZhi"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySBQyV4ui-rT"
   },
   "source": [
    "Finally, just like you've seen before, launch tensorboard. Spend some time going through the different tabs to help familiarize yourself with the interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXMsmRLLxHqL"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir=lightning_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tsS2ofI6Ezf"
   },
   "source": [
    "Take a good look above. Notice that the validation loss increases and continues to rise after a while. That is a clear sign of overfitting, which is not what you want. What you'd like is a way to know when the validation loss stops decreasing and then stop the training. This is known as early stopping. Now let's add early stopping to monitor the `val_loss`. You do this by adding `EarlyStopping` as a callback. This will check for `val_loss`, which is being logged above. If `val_loss` does not decrease for a number of epochs in a row, training will halt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-Ug0u9n6y2V"
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "\n",
    "# $MODIFY 2\n",
    "hidden_layers = [32]\n",
    "# $MODIFY 3\n",
    "activation_func = nn.ReLU()\n",
    "\n",
    "lit_model = LitModel(\n",
    "    Network(hidden_layers, activation_func) \n",
    ")\n",
    "\n",
    "# $MODIFY 4\n",
    "batch_size = 128\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader   = DataLoader(val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    enable_checkpointing=False,\n",
    "    callbacks=[TQDMProgressBar(refresh_rate=100), EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)]\n",
    ")\n",
    "\n",
    "trainer.fit(lit_model, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPbooQ8274Nn"
   },
   "source": [
    "Take note how many epochs it ran for! Let's test the accuracy on the test set to see if it performs well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uN1P8Par7Rwi"
   },
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_set, batch_size=batch_size)\n",
    "trainer.test(lit_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsSgk-VycPkg"
   },
   "source": [
    "### Tasks\n",
    "Below are a list of tasks to try to help you solidify your understanding. Remember, each number corresponds to a `$MODIFY` comment above.\n",
    "\n",
    "1. Try removing the standardization portion and re-running. Do you notice any differences in how the model trains? \n",
    "2. Change the hidden layer structure. A common architecture you'll see is a \"funnel\" approach, where the number of neurons decreases as the layer number increases. Try some different depths, number of neurons per layer and weird architectures. You'll probably want to run this with early stopping. What do you notice about the accuracy? Does the architecture play a role in training time? How do the number of parameters get effectred?\n",
    "3. Try some different [activation functions](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity). How does it effect overall training and accuracy?\n",
    "4. Change the `batch_num`. How does this effect the training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnIZnDeTeyz3"
   },
   "source": [
    "### Summary\n",
    "In this tutorial you learned how to load the training and test data using PyTorch `datasets`. You learned some of the differences between normalizing and standardizing your data, and saw how to use PyTorch `transforms` to accomplish this. You were able to create a deep neural network of arbitrary depth and were able use different activation functions. Finally, you saw how to detect overfitting, and how to prevent it using early stopping.\n",
    "\n",
    "That's it for now. See you next time!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
