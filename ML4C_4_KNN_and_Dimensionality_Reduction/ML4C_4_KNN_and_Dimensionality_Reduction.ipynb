{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensionality Reduction + KNN\n",
        "In the last tutorial I alluded to whether or not we needed to include features that have roughly the same spread of data. That is, if 90% of values in a given feature are the same, is that feature actually important? What we did in the last tutorial with data preprocessing involved a fair bit of manual feature reduction. In this tutorial you will be looking at a couple of ways to automatically reduce the number of features. You will also be learning about a type of classification based on neighboring data."
      ],
      "metadata": {
        "id": "7H03m-VZDKyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST\n",
        "\n",
        "MNIST.... again? Ya, I know you've already seen MNIST in the multiclass classification tutorial, but I think it will be a good one to introduce dimensionality reduction and KNN. In this tutorial you will learn:\n",
        "\n",
        "- KNN\n",
        "- PCA\n",
        "- t-SNE\n",
        "\n",
        "Let's get started!"
      ],
      "metadata": {
        "id": "tKUgA598R2UO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNN\n",
        "\n",
        "K nearest neighbors (KNN) is a pretty simple algorithm. It's actually your first introduction to unsupervised learning as well. Unsupervised learning techniques don't rely on a label. You might be asking, why use it then? After all, we have the labels, so why not use them? KNN is only searching for the neraest points, but we can use it in conjunction with the label to help determine the class label. Let's start with a simple example by creating a 2D space with 9 points and 3 classes."
      ],
      "metadata": {
        "id": "WfVmDqyoS1xn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "points = [\n",
        "    (-2, 2), (-3, 2), (-3, 3), (-2, 3), # Class 0\n",
        "    (-2, -1), (-3, -1), (-3,-2),        # Class 1\n",
        "    (1, 2), (2, 2), (2, 3)              # Class 2\n",
        "]\n",
        "\n",
        "classes = [0, 0, 0, 0, 1, 1, 1, 2, 2, 2]\n",
        "\n",
        "x, y = zip(*points)\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.scatterplot(x=x, y=y, c=classes, s=500)\n"
      ],
      "metadata": {
        "id": "sHqUT0MogK9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's assume a new point comes in and we don't know what class it belongs to. We can make an educated guess that it belongs whatever class it's near. In fact we can make it a bit more robust by taking into account the `k` nearest points. This is KNN. Let's see what this looks like by adding a new point and using KNN to classify which class it should belong to."
      ],
      "metadata": {
        "id": "B4cSYb1pkED5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "import numpy as np\n",
        "\n",
        "new_point = (0, 0)\n",
        "x = np.array(points)\n",
        "\n",
        "f, axs = plt.subplots(4, 2, figsize=(20,40))\n",
        "ks = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "\n",
        "def plot_lines_from_point_to_neighbors(point, neighboring_points, ax, dists):\n",
        "    # Flatten and then grab the x, y coords\n",
        "    neighbors = neighboring_points.ravel()\n",
        "    xn = x[neighbors][:, 0].tolist()\n",
        "    yn = x[neighbors][:, 1].tolist()\n",
        "\n",
        "    # Plot a line segment from from the new point to the neighbor\n",
        "    for p in zip(xn, yn):\n",
        "        src = point\n",
        "        dst = p\n",
        "        ax.plot([src[0], dst[0]], [src[1], dst[1]])\n",
        "    \n",
        "    # Plot distances of all points\n",
        "    for dist, dst in zip(dists, x.tolist()):\n",
        "        ax.text(dst[0]-0.015, dst[1]+0.25, f'{dist:.3f}')\n",
        "\n",
        "\n",
        "for i, k in enumerate(ks):\n",
        "    # Store the training data\n",
        "    knn = NearestNeighbors(n_neighbors=k, metric='euclidean', algorithm='brute')\n",
        "    knn.fit(x)\n",
        "\n",
        "    # Find the nearest points\n",
        "    data_point = np.array(new_point).reshape(-1, 2)\n",
        "    dists, neighbors = knn.kneighbors(data_point, return_distance=True)\n",
        "   \n",
        "    # Grab the most frequent class\n",
        "    neighboring_classes = np.array(classes)[neighbors].ravel()\n",
        "    new_class = np.bincount(neighboring_classes).argmax()\n",
        "\n",
        "    xs = x[:, 0].ravel().tolist() + [new_point[0]]\n",
        "    ys = x[:, 1].ravel().tolist() + [new_point[1]]\n",
        "    row, col = i // 2, i % 2\n",
        "    axs[row, col].scatter(xs, ys, c=classes + [new_class], s=350)\n",
        "    axs[row, col].set_title(f'k={k} predicts {new_class}')\n",
        "\n",
        "    # Compute the distance of the points purely for plotting reasons\n",
        "    dists = np.linalg.norm(x - np.array(new_point), axis=1).tolist()\n",
        "    plot_lines_from_point_to_neighbors(new_point, neighbors, axs[row, col], dists)"
      ],
      "metadata": {
        "id": "xTzv_GSNllsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a look at the above plot. Does it make sense? For the most part it does, but take a look at `k=1` and `k=2`. Notice that in both it predicts class 1 instead of class 2 even though they're the same distance. This is because `numpy` needs a way to break ties when we look for the frequency. In this case it ends up selecting the one with the smaller numerical class. That's probably fine in this case. Now take a look at `k=3`. In this case the 3 nearest neighbors are a neighbor from each class: `0, 1, 2`. It might be a bit hard to tell but class 0 is actually the furthest away, however, because of how `numpy` is breaking ties, it doesn't take that into account. This is also probably fine in the long run. Yes, you might end up with an incorrect classification here or there, but overall this probably won't matter. If it does, you can make a smarter selection by trying to break ties based on the average distance of those neighbors. Here's what that would look like."
      ],
      "metadata": {
        "id": "oEnjsKwYHrIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "def get_class(dists, classes):\n",
        "    df = pd.DataFrame(np.column_stack((dists.T, classes.T)), columns=['dists', 'classes'])\n",
        "    # Create a mapping from class -> avg dist\n",
        "    avg_dists = {curr_class: df.loc[df['classes'] == curr_class, 'dists'].mean() for curr_class in df['classes'].unique().tolist()}\n",
        "    classes = classes.tolist()\n",
        "    count = Counter(classes)\n",
        "    # Sort by smallest negative count, i.e. largest count with tie breaker going\n",
        "    # to the smallest avg distv and then finally just choosing smallest class\n",
        "    sort = sorted(classes, key = lambda c: (-count[c], avg_dists[c], c))\n",
        "    return sort[0]\n",
        "\n",
        "\n",
        "new_point = (0, 0)\n",
        "x = np.array(points)\n",
        "\n",
        "f, axs = plt.subplots(4, 2, figsize=(20,40))\n",
        "ks = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "\n",
        "for i, k in enumerate(ks):\n",
        "    # Store the training data\n",
        "    knn = NearestNeighbors(n_neighbors=k, metric='euclidean', algorithm='brute')\n",
        "    knn.fit(x)\n",
        "\n",
        "    # Find the nearest points\n",
        "    data_point = np.array(new_point).reshape(-1, 2)\n",
        "    dists, neighbors = knn.kneighbors(data_point, return_distance=True)\n",
        "   \n",
        "    # Grab the most frequent class\n",
        "    neighboring_classes = np.array(classes)[neighbors].ravel()\n",
        "    new_class = get_class(dists, neighboring_classes)\n",
        "\n",
        "    xs = x[:, 0].ravel().tolist() + [new_point[0]]\n",
        "    ys = x[:, 1].ravel().tolist() + [new_point[1]]\n",
        "    row, col = i // 2, i % 2\n",
        "    axs[row, col].scatter(xs, ys, c=classes + [new_class], s=350)\n",
        "    axs[row, col].set_title(f'k={k} predicts {new_class}')\n",
        "\n",
        "    # Compute the distance of the points purely for plotting reasons\n",
        "    dists = np.linalg.norm(x - np.array(new_point), axis=1).tolist()\n",
        "    plot_lines_from_point_to_neighbors(new_point, neighbors, axs[row, col], dists)"
      ],
      "metadata": {
        "id": "5BO02vnUI_Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, KNN relies on `k` and how you decide to select the majority class. You may also have noticed that when there are a number of points at the same distance, sklearn will choose one over the other. This is fine since in the long wrong it's unlikely to negatively impact accuracy.\n",
        "\n",
        "Now that you've seen how KNN works, let's apply it on `MNIST`. Rather than having the 2 features (x and y location) that you just saw above, you're now going to have 784 features (28 x 28 pixels). Because of this there are a lot more computations that need to be done in order to determine the nearest neighbors. After all, you're not going to know the nearest ones unless you calculate the distance to all points. Let's see what this looks like."
      ],
      "metadata": {
        "id": "Ck1c6U_Zf77Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
        "\n",
        "X = mnist.data.to_numpy()\n",
        "y = mnist.target.to_numpy()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10000, random_state=0xC0FFEE)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "xJlCE3CzS4Ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start by looking at the type of data we receive."
      ],
      "metadata": {
        "id": "fgHkEzJHitsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('X_train type', X_train.dtype)\n",
        "print('y_train type', y_train.dtype)\n",
        "print('min/max', X_train.min(), X_train.max())"
      ],
      "metadata": {
        "id": "63CsIVrQMuCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case `X_train` is stored as a float with pixel values `[0, 255]` and `y_train` looks to be a string. Let's convert y to an int."
      ],
      "metadata": {
        "id": "1IuHFJAejBNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y_train.astype(np.int32)\n",
        "y_test  = y_test.astype(np.int32)"
      ],
      "metadata": {
        "id": "g5fBvcidjTI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(knn, n, key_metric, X_test):\n",
        "    # Calculate the neighbors for X_test\n",
        "    dists, neighbors = knn.kneighbors(X_test, n_neighbors=n, return_distance=True)\n",
        "    if key_metric == 'maximum_class':\n",
        "        y_pred = [np.bincount(row).argmax() for row in y_train[neighbors]]\n",
        "    else:\n",
        "        # Notice that we look at y_train here. The neighbors will be in reference\n",
        "        # to what data point were provided, which in this case is X_train/y_train\n",
        "        neighboring_classes = y_train[neighbors]\n",
        "        y_pred = [get_class(dists[i], neighboring_classes[i]) for i in range(len(neighboring_classes))]\n",
        "    return (y_pred == y_test).mean()"
      ],
      "metadata": {
        "id": "xgk_U2BnuWh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
        "import time\n",
        "\n",
        "neighbors = [3, 5, 15, 101]\n",
        "time_taken = {\n",
        "    'maximum_class': [],\n",
        "    'maximum_class_min_distance': []\n",
        "}\n",
        "accuracies = {\n",
        "    'maximum_class': [],\n",
        "    'maximum_class_min_distance': []\n",
        "}\n",
        "\n",
        "def train_knns():\n",
        "    for n in neighbors:\n",
        "        for key in time_taken.keys():\n",
        "            print(f'training with {n} neighbors and {key} classification')\n",
        "            knn = NearestNeighbors(n_neighbors=n)\n",
        "            knn.fit(X_train)\n",
        "            start = time.time()\n",
        "            accuracy = get_accuracy(knn, n, key, X_test)\n",
        "            end = time.time()\n",
        "            print(f'time: {(end-start)/60:.3f} min\\t\\tAccuracy: {(100*accuracy):.2f}')\n",
        "\n",
        "            time_taken[key].append(end - start)\n",
        "            accuracies[key].append(accuracy)\n",
        "\n",
        "train_knns()"
      ],
      "metadata": {
        "id": "yNLpt6MTTiff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.DataFrame.from_dict(time_taken)\n",
        "df2 = pd.DataFrame.from_dict(accuracies)\n",
        "\n",
        "\n",
        "f, ax = plt.subplots(figsize=(10,10))\n",
        "sns.lineplot(x=neighbors, y=df2['maximum_class'])\n",
        "sns.lineplot(x=neighbors, y=df2['maximum_class_min_distance'])\n",
        "ax.set_ylabel('Accuracy', fontdict={'fontsize': 20})\n",
        "# Share the same X\n",
        "ax2 = ax.twinx()\n",
        "ax2.set_ylabel('Time taken (s)', fontdict={'fontsize': 20})\n",
        "sns.lineplot(x=neighbors, y=df1['maximum_class'], ax=ax2)\n",
        "sns.lineplot(x=neighbors, y=df1['maximum_class_min_distance'], ax=ax2)\n",
        "\n",
        "ax.set_xlabel('K-neighbors', fontdict={'fontsize': 20})\n",
        "ax.tick_params(axis='both', which='major', labelsize=15)\n",
        "ax2.tick_params(axis='both', which='major', labelsize=15)\n",
        "plt.legend(labels=['max class', 'max class + min dist'], fontsize=15, loc='lower right')"
      ],
      "metadata": {
        "id": "sIb_6c3iTB31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the accuracies are pretty close, but using the custom metric for determining the label can be quite a bit slower. So is that it? Are we done? Nope! Let's look at two numbers in the dataset to gain intuition as to why we might want to reduce the dimensions."
      ],
      "metadata": {
        "id": "Tso7Bn2ykiY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 10))\n",
        "plt.gray()\n",
        "\n",
        "ax1.imshow(X_train[0].reshape(28, 28))\n",
        "ax2.imshow(X_train[1].reshape(28, 28))\n"
      ],
      "metadata": {
        "id": "Z43LK5kRlFpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each pixel is it's own feature. If every image has a black pixel at (0, 0) then it's not a necessary feature to include. In fact, we're only really concerned with pixels (features) that have a high degree of variability since this will contain the most amount of information. We'll use something called principal components analysis (PCA) to automatically do this for us. PCA is a linear reduction technique that tries to project a feature space onto a smaller feature space in the direction that preserves maximum variance. That probably doesn't make much sense so I'll use a 3D ellipsoid to help explain. Below is code to plot a 3D ellipsoid, you can ignore it if you want."
      ],
      "metadata": {
        "id": "eWmVrsRhmO7d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg1JTe18M10Z"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a meshgrid for the space\n",
        "nsamples = 100\n",
        "x_radius, y_radius, z_radius = 20, 4, 6\n",
        "space = np.linspace(0, 2 * np.pi, nsamples)\n",
        "u, v = np.meshgrid(space, space)\n",
        "# Create the x,y,z locations across the space\n",
        "x = x_radius * np.cos(u) * np.cos(v)\n",
        "y = y_radius * np.cos(u) * np.sin(v)\n",
        "z = z_radius * np.sin(u)\n",
        "\n",
        "# Stack X such that the columns are (x, y, z)\n",
        "X = np.hstack((\n",
        "    x.reshape(-1, 1),\n",
        "    y.reshape(-1, 1),\n",
        "    z.reshape(-1, 1) \n",
        "))\n",
        "\n",
        "fig = plt.figure(figsize=(15,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "ax.scatter(X[:, 0], X[:, 1], X[:, 2])\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above is the 3D ellipsoid. It has 3 features/axes. The x-axis has a radius of 20, the y-axis a radius of 4 and the z-axis a radius of 6. Let's say that we wanted to project this 3D space to 2D such that we still have the most amount of information on this object as possible. You'd want to get rid of the y-axis and just have the x and z-axis. This way you retain the most information. Let's see this in action by performing PCA on this object down to 2 features."
      ],
      "metadata": {
        "id": "bmLwzB8Co-zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(X)\n",
        "X_pca = pca.transform(X)"
      ],
      "metadata": {
        "id": "sCBNOg9WQi11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = plt.figure(figsize=(10,5))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LmbvKjM9Q_ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretty cool, right? PCA was automatically able to project to a lower dimensioned space that maintained the most amount of overall variance. Now let's do PCA once more to go down to a single feature/axis. What do you think will happen? Will there be a line of length 12 or a line of length 40? Pause for a moment and then run the below cell."
      ],
      "metadata": {
        "id": "AsPhmI7aqfIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "pca = PCA(n_components=1)\n",
        "pca.fit(X)\n",
        "X_pca = pca.transform(X)\n",
        "plt.scatter(X_pca[:, 0], np.zeros(X_pca.shape))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GqzuozInUuVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is that what you expected? Since that axis had the most variance, it's the one you end up with. You can check the explained variance ratio as well to see the percentance of variance explained by each component that remains. In this case you'll have only one entry."
      ],
      "metadata": {
        "id": "VrxxrWRAq_8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca.explained_variance_ratio_"
      ],
      "metadata": {
        "id": "Xk8oIebMrS4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generally a rule of thumb is ~80% explained variance. Let's use this to construct a PCA for the data."
      ],
      "metadata": {
        "id": "9io3PbbDsJ9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(0.80, svd_solver='full')\n",
        "pca.fit(X_train / 255.0)\n",
        "X_train_pca = pca.transform(X_train / 255.0)"
      ],
      "metadata": {
        "id": "YOC_wAiSsXaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check how many components (features) we ended up with."
      ],
      "metadata": {
        "id": "ah1bPv1Fsx-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('num components', pca.n_components_)\n",
        "print('explained variance', pca.explained_variance_ratio_.sum())\n"
      ],
      "metadata": {
        "id": "oSAw_AwVsmrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From 784 features down to only 43! Let's see how the accuracies compare using KNN."
      ],
      "metadata": {
        "id": "0CNDZ4sFs8v9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neighbors = [3, 5, 15, 101]\n",
        "X_test_pca = pca.transform(X_test / 255.0)\n",
        "\n",
        "pca_accuracies = []\n",
        "for k in neighbors:\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(X_train_pca)\n",
        "    start = time.time()\n",
        "    accuracy = get_accuracy(knn, k, 'maximum_class', X_test_pca)\n",
        "    end = time.time()\n",
        "\n",
        "    pca_accuracies.append(accuracy)\n",
        "    print(f'{k} nearest: {accuracy*100:.3f}')\n"
      ],
      "metadata": {
        "id": "vdPmsjuLs6TG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how these compared to the old accuracies of maximum class."
      ],
      "metadata": {
        "id": "hf1_maDnxp5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_acc = pd.concat((\n",
        "    df2,\n",
        "    pd.DataFrame(pca_accuracies, columns=['pca_acc'])), axis=1\n",
        ")\n",
        "df_acc = df_acc[['maximum_class', 'pca_acc']]\n",
        "\n",
        "f, ax = plt.subplots(figsize=(10, 10))\n",
        "sns.lineplot(x=neighbors, y=df_acc['maximum_class'])\n",
        "sns.lineplot(x=neighbors, y=df_acc['pca_acc'])\n",
        "ax.set_xlabel('K-neighbors', fontdict={'fontsize': 20})\n",
        "ax.set_ylabel('Accuracy', fontdict={'fontsize': 20})\n",
        "ax.legend(labels=['max class', 'pca'], fontsize=15, loc='lower right')\n",
        "ax.tick_params(axis='both', which='major', labelsize=15)"
      ],
      "metadata": {
        "id": "_YHB_J96xuJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, PCA actually performs better in terms of accuracy than using all the features. That's awesome! We get a smaller memory footprint, faster runtime since there are fewer features to calculate distance, and better accuracy. One of the nice things about going down to a smaller dimensionality is you can plot it and see the distribution of data. Let's start by seeing what accuracies we get with just 2 features. Also notice that we're normalizing the data before putting it into PCA above. This isn't especially necessary for this case since the features have the same range. Since PCA works by projeting to the direction with maximum variance, if one feature has a larger range, it might get incorrectly marked as more important. "
      ],
      "metadata": {
        "id": "PAB5FAPG8bx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "pca.fit(X_train / 255.0)\n",
        "X_train_pca = pca.transform(X_train / 255.0)\n",
        "\n",
        "neighbors = [3, 5, 15, 101]\n",
        "X_test_pca = pca.transform(X_test / 255.0)\n",
        "\n",
        "pca_accuracies = []\n",
        "for k in neighbors:\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(X_train_pca)\n",
        "    start = time.time()\n",
        "    accuracy = get_accuracy(knn, k, 'maximum_class', X_test_pca)\n",
        "    end = time.time()\n",
        "\n",
        "    pca_accuracies.append(accuracy)\n",
        "    print(f'{k} nearest: {accuracy*100:.3f}')\n"
      ],
      "metadata": {
        "id": "Wfg3325U-mdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case it actually benefits to have more neighbors, where previously it didn't. You can also see the accuracy is quite low. Let's take a look at this transformed data and see what we're getting."
      ],
      "metadata": {
        "id": "xG2syzMDDRIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(9,9))\n",
        "sns.scatterplot(x=X_train_pca[:1000, 0], y=X_train_pca[:1000, 1], hue=y_train[:1000],\n",
        "                palette=sns.color_palette(\"hls\", 10), s=200)\n"
      ],
      "metadata": {
        "id": "NkMcd8lvAdN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above is the first 1000 data points. You can see they overlap quite a bit, so KNN will probably struggle here. This makes sense if you think about it. We initially had 784 features and went down to 2... so a lot of information was lost. Let's look at the explained variance."
      ],
      "metadata": {
        "id": "0MW-SEknDf11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca.explained_variance_ratio_.sum()"
      ],
      "metadata": {
        "id": "eYgBvbrvEGuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previously we had 80% variance explained, and now only ~17%. So doing continuous linear projections down to 2 dimensions lost us a lot of information. Let's try a non-linear projection called `T-SNE`. T-SNE is not a linear reduction technique and won't be able to tell us a variance ratio."
      ],
      "metadata": {
        "id": "dgOqco9RELnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Create a subsample\n",
        "X_sub = X_train[:1500, :] / 255.0\n",
        "\n",
        "\n",
        "X_train_tsne = TSNE(learning_rate='auto', perplexity=30,\n",
        "                    init='pca').fit_transform(X_sub)\n"
      ],
      "metadata": {
        "id": "6j8xhATXEKnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's plot the results of T-SNE."
      ],
      "metadata": {
        "id": "HIaYgWDKR5zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "sns.scatterplot(x=X_train_tsne[:, 0], y=X_train_tsne[:, 1], hue=y_train[:1500],\n",
        "                palette=sns.color_palette(\"hls\", 10), s=200)\n",
        "plt.legend(markerscale=2)\n"
      ],
      "metadata": {
        "id": "--V1PKHtIwJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This actually looks like it's formed some distinguishable clusters! There's still some overlap, but it's looking a lot better. One problem with t-SNE is there is only a `fit_transform()` and no `transform()`. This isn't just an issue with sklearn's library. This is because t-SNE doesn't learn a mapping from high dimensional space to lower dimensional space. The way it learns is beyond the scope of this tutorial, but it's an iterative algorithm that tries to minimize some distance between the two spaces. It's directly learning with the data given and modifies the points in the low dimensional space. Because of this there isn't a way to really perform `transform()`. One way to get around this is to pass in the test data with the training data, then you can transform both at once. Let's see what this might look like on a subset of the training/test data with KNN."
      ],
      "metadata": {
        "id": "Ur0aEvo1SF9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ntrain = 10000\n",
        "X_train_sub = X_train[:ntrain, :] / 255.0\n",
        "X_test_sub = X_test[:1500, :] / 255.0\n",
        "# Combine into one\n",
        "X_sub = np.row_stack((X_train_sub, X_test_sub))\n",
        "\n",
        "# $MODIFY 1\n",
        "tsne = TSNE(n_components=2, learning_rate='auto', perplexity=30,\n",
        "                    init='pca').fit_transform(X_sub)"
      ],
      "metadata": {
        "id": "3UOJztyqaNyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can run KNN."
      ],
      "metadata": {
        "id": "-4UAJ4sJC0u4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neighbors = [3, 5, 15, 101]\n",
        "\n",
        "for k in neighbors:\n",
        "    knn = NearestNeighbors(n_neighbors=k)\n",
        "    knn.fit(tsne[:ntrain])  # Add just the training data\n",
        "    # Get accuracy on the test data\n",
        "    neighbors = knn.kneighbors(tsne[ntrain:], n_neighbors=k, return_distance=False)\n",
        "    y_pred = [np.bincount(row).argmax() for row in y_train[neighbors]]\n",
        "    accuracy = (y_pred == y_test[:1500]).mean()\n",
        "    print(f'KNN {k} with tsne - accuracy: {(accuracy*100):.3f}%')"
      ],
      "metadata": {
        "id": "OLBpYCi7badl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tasks\n",
        "\n",
        "1. Change the `n_components` of t-SNE. How do you find it effects your accuracy?\n",
        "\n",
        "- What are some pros and cons of KNN?\n",
        "- What are some pros and cons of t-SNE?"
      ],
      "metadata": {
        "id": "V1GSfcwrqWgi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "In this tutorial you saw how KNN works and how to use it. You also learned how to use KNN as a classifier by simply looking at the training labels. You then learned about PCA and how to apply it to your data. You saw that you can get good results when you have a good explained variance ratio, but linearly projecting down too many features results in poor accuracy. Finally you learned about t-SNE and how you can use it to do a non-linear projection down to even just two features, which can be quite helpful for visualizing your data.\n",
        "\n",
        "That's all for this one, see you next time!"
      ],
      "metadata": {
        "id": "14daGIsCDIiI"
      }
    }
  ]
}